{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Data: `Dataset`, `DataLoader`, `Sampler`, and `Transforms`\n",
    "\n",
    "These basic concepts make it easy to work with large data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init, helpers, utils, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import utils  # little helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "It's easy to create your `Dataset`,\n",
    "but PyTorch comes with some\n",
    "[build-in datasets](https://pytorch.org/docs/stable/torchvision/datasets.html):\n",
    "\n",
    "- MNIST\n",
    "- Fashion-MNIST\n",
    "- KMNIST\n",
    "- EMNIST\n",
    "- FakeData\n",
    "- COCO\n",
    "  - Captions\n",
    "  - Detection\n",
    "- LSUN\n",
    "- ImageFolder\n",
    "- DatasetFolder\n",
    "- Imagenet-12\n",
    "- CIFAR\n",
    "- STL10\n",
    "- SVHN\n",
    "- PhotoTour\n",
    "- SBU\n",
    "- Flickr\n",
    "- VOC\n",
    "- Cityscapes\n",
    "\n",
    "`Dataset` gives you information about the number of samples (implement `__len__`) and gives you the sample at a given index (implement `__getitem__`.\n",
    "It's a nice and simple abstraction to work with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Dataset(object):\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ImageFolder` dataset is quite useful and follows the usual conventions for folder layouts:\n",
    "\n",
    "```\n",
    "root/dog/xxx.png\n",
    "root/dog/xxy.png\n",
    "root/dog/xxz.png\n",
    "\n",
    "root/cat/123.png\n",
    "root/cat/nsdf3.png\n",
    "root/cat/asd932_.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = utils.DogsCatsDataset(\"../data/raw\", \"sample/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -d ../data/raw/dogscats/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the __len__ method\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the __getitem__ method\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[14][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[14][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, some datasets offer convenience functions and attributes.\n",
    "This is not enforced by the interface! Don't rely on it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label_id in random.sample(list(train_ds), 4):\n",
    "    print(label_id, train_ds.classes[label_id])\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `torchvision.transforms`\n",
    "\n",
    "Common image transformation that can be composed/chained [[docs]](https://pytorch.org/docs/stable/torchvision/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_image_size = 224\n",
    "_mean = [0.485, 0.456, 0.406]\n",
    "_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.RandomCrop(_image_size),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ColorJitter(.3, .3, .3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])\n",
    "\n",
    "trans(train_ds[13][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torchvision.transforms.functional`\n",
    "\n",
    ">Functional transforms give you fine-grained control of the transformation pipeline. As opposed to the transformations above, functional transforms donâ€™t contain a random number generator for their parameters. That means you have to specify/generate all parameters, but you can reuse the functional transform. For example, you can apply a functional transform to multiple images like this:\n",
    ">\n",
    "> https://pytorch.org/docs/stable/torchvision/transforms.html#functional-transforms\n",
    "\n",
    "```python\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "def my_segmentation_transforms(image, segmentation):\n",
    "    if random.random() > 5:\n",
    "        angle = random.randint(-30, 30)\n",
    "        image = TF.rotate(image, angle)\n",
    "        segmentation = TF.rotate(segmentation, angle)\n",
    "    # more transforms ...\n",
    "    return image, segmentation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:\n",
    "- https://pytorch.org/docs/stable/torchvision/transforms.htm\n",
    "- https://pytorch.org/docs/stable/torchvision/transforms.html#functional-transforms\n",
    "- https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "- https://github.com/mdbloice/Augmentor\n",
    "- https://github.com/aleju/imgaug\n",
    "\n",
    "Shout-out:\n",
    "- Hig performance image augmentation with pillow-simd [[github]](https://github.com/uploadcare/pillow-simd) [[benchmark]](http://python-pillow.org/pillow-perf/)\n",
    "- Improving Deep Learning Performance with AutoAugment [[blog]](https://ai.googleblog.com/2018/06/improving-deep-learning-performance.html) [[paper]](https://arxiv.org/abs/1805.09501) [[pytorch implementation]](https://github.com/DeepVoltaire/AutoAugment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "The `DataLoader` class offers batch loading of datasets with multi-processing and different sample strategies [[docs]](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "The signature looks something like this:\n",
    "```python\n",
    "DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    batch_sampler=None,\n",
    "    num_workers=0,\n",
    "    collate_fn=default_collate,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    timeout=0,\n",
    "    worker_init_fn=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = utils.DogsCatsDataset(\"../data/raw\", \"sample/train\", transform=trans)\n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_dl)\n",
    "X, y = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I passed `trans`, which returns `torch.Tensor`, not pillow images.\n",
    "DataLoader expects tensors, numbers, dicts or lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_ds = utils.DogsCatsDataset(\"../data/raw\", \"sample/train\", transform=None) \n",
    "_train_dl = DataLoader(_train_ds, batch_size=2, shuffle=True)\n",
    "\n",
    "try:\n",
    "    for batch in _train_dl:\n",
    "        pass\n",
    "except TypeError as e:\n",
    "    print(\"ERROR\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `collate_fn`\n",
    "The `collate_fn` argument of `DataLoader` allows you to customize how single datapoints are put together into a batch.\n",
    "`collate_fn` is a simple callable that gets a list of datapoints (i.e. what `dataset.__getitem__` returns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a custom `collate_fn`\n",
    "(taken from [here](https://discuss.pytorch.org/t/how-to-create-a-dataloader-with-variable-size-input/8278/3)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(list_of_x_y):\n",
    "    data = [item[0] for item in list_of_x_y]\n",
    "    target = [item[1] for item in list_of_x_y]\n",
    "    target = torch.LongTensor(target)\n",
    "    return [data, target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampler\n",
    "`Sampler` define **how** to sample from the dataset [[docs]](https://pytorch.org/docs/stable/data.html#torch.utils.data.sampler.Sampler).\n",
    "\n",
    "Examples:\n",
    "- `SequentialSampler`\n",
    "- `RandomSamples`\n",
    "- `SubsetSampler`\n",
    "- `WeightedRandomSampler`\n",
    "\n",
    "Write your own by simply implementing `__iter__` to iterate over the indices of the dataset.\n",
    "\n",
    "```python\n",
    "class Sampler(object):\n",
    "    def __init__(self, data_source):\n",
    "        pass\n",
    "\n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "- `Dataset`: get one datapoint\n",
    "- `transforms`: composable transformations\n",
    "- `DataLoader`: combine single datapoints into batches (plus multi processing and more)\n",
    "- `Sampler`: **how** to sample from a dataset\n",
    "\n",
    "**Simple but extensible interfaces**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Go out and play:\n",
    "\n",
    "- Maybe extend the `DogsCatsDataset` such that you can specify the size of dataset, i.e. the number of samples.\n",
    "- Maybe try the `Subset` [[docs]](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset) to create smaller datasets.\n",
    "- Maybe create `SubsetFraction` where you can specify the size of the dataset (between 0. and 1.).\n",
    "- Maybe write a custom collate function for the `DogsCatsDataset` that turns it into a dataset appropriate to use in an autoencoder settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_collate_fn(list_of_x_y):\n",
    "    # TODO implement me\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO implement me\n",
    "    \n",
    "    def __len__(self):\n",
    "        # TODO implement me\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO implement me\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level training libraries\n",
    "\n",
    "Writing the training loop is my least favourite thing about PyTorch.\n",
    "\n",
    "Keras is great here!\n",
    "```python\n",
    "model.compile(optimizer, criterion, metrics=[\"accuracy\", \"f1\"])\n",
    "model.fit(X, y, epochs=10)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init, helpers, utils, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    get_model,\n",
    "    get_data,\n",
    ")\n",
    "from my_train_helper import get_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/raw/dogscats/sample/train.\n",
      "Loading data from ../data/raw/dogscats/sample/valid.\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Contenders\n",
    "## [Ignite](https://github.com/pytorch/ignite)\n",
    "> Ignite is a high-level library to help with training neural networks in PyTorch.\n",
    "> - ignite helps you write compact but full-featured training loops in a few lines of code\n",
    "> - you get a training loop with metrics, early-stopping, model checkpointing and other features without the boilerplate\n",
    "\n",
    "\n",
    "## [TNT](https://github.com/pytorch/tnt)\n",
    "> TNT is a library providing powerful dataloading, logging and visualization utlities for Python. It is closely intergrated with PyTorch and is designed to enable rapid iteration with any model or training regimen.\n",
    "> [...]\n",
    "> The project was inspired by TorchNet, and legend says that it stood for “TorchNetTwo”\n",
    "\n",
    "\n",
    "## [Skorch](https://github.com/dnouri/skorch)\n",
    "> A scikit-learn compatible neural network library that wraps PyTorch.\n",
    "\n",
    "\n",
    "## [Poutyne](https://poutyne.org/) (formally PyToune)\n",
    "> Poutyne is a Keras-like framework for PyTorch and handles much of the boilerplating code needed to train neural networks.\n",
    "\n",
    "\n",
    "## [fast.ai](https://docs.fast.ai/)\n",
    "> The fastai library simplifies training fast and accurate neural nets using modern best practices. It's based on research in to deep learning best practices undertaken at fast.ai, including \"out of the box\" support for vision, text, tabular, and collab (collaborative filtering) models.\n",
    "\n",
    "\n",
    "## [MagNet](https://github.com/MagNet-DL/magnet)\n",
    "> MagNet is a high-level Deep Learning API, wrapped around PyTorch.\n",
    "> \n",
    "> It was developed with the aim of reducing boilerplate code and writing Deep Learning architectures with more grace.\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(mn.Conv(32), *mn.Conv() * 2, mn.Linear(10, act=None))\n",
    "```\n",
    "\n",
    "## \"The Fun of Reinvention\"\n",
    "Clearly, there must be a better way! Write your own lib (but don't use it) :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignite Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ignite\n",
    "from ignite.metrics import (\n",
    "    Accuracy,\n",
    "    Loss,\n",
    "    Precision,\n",
    ")\n",
    "from ignite.engine import (\n",
    "    create_supervised_evaluator,\n",
    "    create_supervised_trainer,\n",
    "    Events,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, loss, optimizer\n",
    "model = get_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    get_trainable(model.parameters()),\n",
    "    lr=0.001,\n",
    "    momentum=.9,\n",
    ")\n",
    "\n",
    "# trainer and evaluator\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=DEVICE)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\n",
    "        \"accuracy\": Accuracy(),\n",
    "        \"loss\": Loss(criterion),\n",
    "        \"precision\": Precision(),\n",
    "    },\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    print(\n",
    "        f\"Epoch[{engine.state.epoch}] \"\n",
    "        f\"Batch[{engine.state.iteration}] \"\n",
    "        f\"Loss: {engine.state.output:.2f}\"\n",
    "    )\n",
    "\n",
    "# trainer.run(train_dl, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Batch[1] Loss: 0.55\n",
      "Epoch[1] Batch[2] Loss: 1.11\n",
      "Epoch[1] Batch[3] Loss: 0.63\n",
      "Epoch[1] Batch[4] Loss: 1.12\n",
      "Epoch[1] Batch[5] Loss: 0.87\n",
      "Epoch[1] Batch[6] Loss: 0.69\n",
      "Epoch[1] Batch[7] Loss: 0.62\n",
      "Epoch[1] Batch[8] Loss: 0.41\n",
      "Training Results   - Epoch: 1  accuracy: 0.88 loss: 0.51 prec: tensor([0.8750, 0.8750], dtype=torch.float64)\n",
      "Validation Results - Epoch: 1  accuracy: 0.88 loss: 0.39 prec: tensor([0.8000, 1.0000], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7f74220674a8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\n",
    "        f\"Training Results   - Epoch: {trainer.state.epoch}  \"\n",
    "        f\"accuracy: {metrics['accuracy']:.2f} \"\n",
    "        f\"loss: {metrics['loss']:.2f} \"\n",
    "        f\"prec: {metrics['precision'].cpu()}\"\n",
    "    )\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\n",
    "        f\"Validation Results - Epoch: {trainer.state.epoch}  \"\n",
    "        f\"accuracy: {metrics['accuracy']:.2f} \"\n",
    "        f\"loss: {metrics['loss']:.2f} \"\n",
    "        f\"prec: {metrics['precision'].cpu()}\"\n",
    "    )\n",
    "\n",
    "\n",
    "trainer.run(train_dl, max_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "- Try out PyToune.\n",
    "- Try out another library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import poutyne\n",
    "from poutyne.framework import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/raw/dogscats/sample/train.\n",
      "Loading data from ../data/raw/dogscats/sample/valid.\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl = get_data()\n",
    "\n",
    "module = get_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    get_trainable(module.parameters()),\n",
    "    lr=0.001,\n",
    "    momentum=.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need two lines of code\n",
    "# model = Model(...)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization with Tensorboard\n",
    "- https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard\n",
    "- https://pytorch.org/docs/stable/tensorboard.html\n",
    "\n",
    "![](img/tensorboardx_demo.gif)\n",
    "\n",
    "Make sure tensorboard and tensorboardx are installed:\n",
    "```\n",
    "conda install tensorboard\n",
    "```\n",
    "\n",
    "\n",
    "Start tensorboard:\n",
    "```\n",
    "cd notebooks\n",
    "tensorboard --logdir=tf_log\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf tf_log/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls tf_log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "summary_writer = SummaryWriter(log_dir=f\"tf_log/demo_{random.randint(0, 1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdemo_348\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls tf_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some scalars\n",
    "for i in range(10):\n",
    "    summary_writer.add_scalar(\"training/loss\", np.random.rand(), i)\n",
    "    summary_writer.add_scalar(\"validation/loss\", np.random.rand() + .1, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then visit http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph/network\n",
    "X, _ = next(iter(train_dl))\n",
    "summary_writer.add_graph(model.cpu(), X.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then visit http://localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tensorboard with ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7f7429df2588>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "# new SummaryWriter for new experiment\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "summary_writer = SummaryWriter(log_dir=f\"tf_log/exp_ignite_{now}\")\n",
    "\n",
    "# Basic setup: model, loss, optimizer\n",
    "model = get_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(get_trainable(model.parameters()), lr=0.0001, momentum=.9)\n",
    "\n",
    "# trainer and evaluator\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=DEVICE)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\"accuracy\": Accuracy(), \"loss\": Loss(criterion)},\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    epoch = trainer.state.epoch\n",
    "    summary_writer.add_scalar(\"training/accuracy\", metrics['accuracy'], epoch)\n",
    "    summary_writer.add_scalar(\"training/loss\", metrics['loss'], epoch)\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    epoch = trainer.state.epoch\n",
    "    summary_writer.add_scalar(\"validation/accuracy\", metrics['accuracy'], epoch)\n",
    "    summary_writer.add_scalar(\"validation/loss\", metrics['loss'], epoch)\n",
    "    print(metrics['accuracy'])\n",
    "\n",
    "\n",
    "trainer.run(train_dl, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization with Visdom\n",
    "https://github.com/facebookresearch/visdom\n",
    "![](img/visdom.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "- Use visdom with ignite.\n",
    "- Use visdom with any other library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
